{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oL2U8NuZ-gLb"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:pass\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import array, argmax, random, take\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from keras.models import Sequential # seqence to sequence model\n",
    "from keras.layers import Dense, LSTM, GRU, Embedding, RepeatVector # different model layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2028,
     "status": "ok",
     "timestamp": 1605980012944,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": -180
    },
    "id": "M85A4rSy-gSD",
    "outputId": "f59d897d-d05a-4e67-8ddb-4412ce7ea02e"
   },
   "outputs": [],
   "source": [
    "path_to_file = \"amha_wol_nmt.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-MNSK1df-Xz"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mXQwKxfq-gX9"
   },
   "outputs": [],
   "source": [
    "def normalize_amharic(amharic_sentence):\n",
    "  # replacing everything with space except those important in amharic language\n",
    "  amharic_sentence = re.sub(r'[^\\u1200-\\u1399?7.!,¿]+', ' ', amharic_sentence)\n",
    "  #remove all ascii characters and Arabic and Amharic numbers\n",
    "  amharic_sentence = re.sub('[\\!\\@\\#\\$\\%\\^\\«\\»\\&\\*\\(\\)\\…\\[\\]\\{\\}\\;\\›\\:\\,\\.\\‹\\/\\<\\>\\?\\\\\\\\|\\~\\-\\=\\+\\፡\\።\\፤\\;\\፦\\፥\\፧\\፨\\፠\\፣]', '',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ሃኅኃሐሓኻ]','ሀ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ሑኁዅ]','ሁ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ኂሒኺ]','ሂ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ኌሔዄ]','ሄ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ሕኅ]','ህ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ኆሖኾ]','ሆ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ሠ]','ሰ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ሡ]','ሱ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ሢ]','ሲ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ሣ]','ሳ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ሤ]','ሴ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ሥ]','ስ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ሦ]','ሶ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ዓኣዐ]','አ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ዑ]','ኡ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ዒ]','ኢ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ዔ]','ኤ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ዕ]','እ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ዖ]','ኦ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ጸ]','ፀ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ጹ]','ፁ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ጺ]','ፂ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ጻ]','ፃ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ጼ]','ፄ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ጽ]','ፅ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ጾ]','ፆ',amharic_sentence)\n",
    "#Normalizing amharic_sentence words with Labialized Amharic characters such as በልቱዋል or  በልቱአል to  በልቷል  \n",
    "  amharic_sentence=re.sub('(ሉ[ዋአ])','ሏ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ሙ[ዋአ])','ሟ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ቱ[ዋአ])','ቷ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ሩ[ዋአ])','ሯ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ሱ[ዋአ])','ሷ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ሹ[ዋአ])','ሿ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ቁ[ዋአ])','ቋ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ቡ[ዋአ])','ቧ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ቹ[ዋአ])','ቿ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ሁ[ዋአ])','ኋ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ኑ[ዋአ])','ኗ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ኙ[ዋአ])','ኟ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ኩ[ዋአ])','ኳ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ዙ[ዋአ])','ዟ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ጉ[ዋአ])','ጓ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ደ[ዋአ])','ዷ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ጡ[ዋአ])','ጧ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ጩ[ዋአ])','ጯ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ጹ[ዋአ])','ጿ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('(ፉ[ዋአ])','ፏ',amharic_sentence)\n",
    "  amharic_sentence=re.sub('[ቊ]','ቁ',amharic_sentence) \n",
    "  amharic_sentence=re.sub('[ኵ]','ኩ',amharic_sentence)\n",
    "  amharic_sentence = amharic_sentence.rstrip().strip()\n",
    "  amharic_sentence = '<start> ' + amharic_sentence + ' <end>'\n",
    "  return amharic_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "20Du7Qcf0uSX"
   },
   "outputs": [],
   "source": [
    "def normalize_wolaita(wolaita_sentence):\n",
    "    #step-1 Convert text to lowercase\n",
    "    wolaita_sentence=wolaita_sentence.lower()\n",
    "    #step-2 Punctuation removal and numbers removal from 0-6 and 8-9\n",
    "    wolaita_sentence = re.sub(r\"[^a-zA-Z?7.!,]+\", \" \", wolaita_sentence)\n",
    "    #step-3 replace several spaces in wolaita_sentence with one space\n",
    "    wolaita_sentence = re.sub(r'[\" \"]+', \" \", wolaita_sentence)\n",
    "    wolaita_sentence = wolaita_sentence.rstrip().strip()\n",
    "    # adding a start and an end token to the sentence # so that the model know when to start and stop predicting.\n",
    "    wolaita_sentence = '<start> ' + wolaita_sentence + ' <end>'\n",
    "    return wolaita_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1886,
     "status": "ok",
     "timestamp": 1605002692568,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "PwA9WGLXU2YQ",
    "outputId": "b8f19bbd-1f55-47e4-becd-92bb4026cc71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ሀ ሁ ሂ ሃ ሄ ህ ሆ ሇ ለ ሉ ሊ ላ ሌ ል ሎ ሏ ሐ ሑ ሒ ሓ ሔ ሕ ሖ ሗ መ ሙ ሚ ማ ሜ ም ሞ ሟ ሠ ሡ ሢ ሣ ሤ ሥ ሦ ሧ ረ ሩ ሪ ራ ሬ ር ሮ ሯ ሰ ሱ ሲ ሳ ሴ ስ ሶ ሷ ሸ ሹ ሺ ሻ ሼ ሽ ሾ ሿ ቀ ቁ ቂ ቃ ቄ ቅ ቆ ቇ ቈ ቊ ቋ ቌ ቍ ቐ ቑ ቒ ቓ ቔ ቕ ቖ ቘ ቚ ቛ ቜ ቝ በ ቡ ቢ ባ ቤ ብ ቦ ቧ ቨ ቩ ቪ ቫ ቬ ቭ ቮ ቯ ተ ቱ ቲ ታ ቴ ት ቶ ቷ ቸ ቹ ቺ ቻ ቼ ች ቾ ቿ ኀ ኁ ኂ ኃ ኄ ኅ ኆ ኇ ኈ ኊ ኋ ኌ ኍ ነ ኑ ኒ ና ኔ ን ኖ ኗ ኘ ኙ ኚ ኛ ኜ ኝ ኞ ኟ አ ኡ ኢ ኣ ኤ እ ኦ ኧ ከ ኩ ኪ ካ ኬ ክ ኮ ኯ ኰ ኲ ኳ ኴ ኵ ኸ ኹ ኺ ኻ ኼ ኽ ኾ ዀ ዂ ዃ ዄ ዅ ወ ዉ ዊ ዋ ዌ ው ዎ ዏ ዐ ዑ ዒ ዓ ዔ ዕ ዖ ዘ ዙ ዚ ዛ ዜ ዝ ዞ ዟ ዠ ዡ ዢ ዣ ዤ ዥ ዦ ዧ የ ዩ ዪ ያ ዬ ይ ዮ ዯ ደ ዱ ዲ ዳ ዴ ድ ዶ ዷ ዸ ዹ ዺ ዻ ዼ ዽ ዾ ዿ ጀ ጁ ጂ ጃ ጄ ጅ ጆ ጇ ገ ጉ ጊ ጋ ጌ ግ ጎ ጏ ጐ ጒ ጓ ጔ ጕ ጘ ጙ ጚ ጛ ጜ ጝ ጞ ጟ ጠ ጡ ጢ ጣ ጤ ጥ ጦ ጧ ጨ ጩ ጪ ጫ ጬ ጭ ጮ ጯ ጰ ጱ ጲ ጳ ጴ ጵ ጶ ጷ ጸ ጹ ጺ ጻ ጼ ጽ ጾ ጿ ፀ ፁ ፂ ፃ ፄ ፅ ፆ ፇ ፈ ፉ ፊ ፋ ፌ ፍ ፎ ፏ ፐ ፑ ፒ ፓ ፔ ፕ ፖ ፗ ፘ ፙ ፚ ፝ ፞ ፟ ፠ ፡ ። ፣ ፤ ፥ ፦ ፧ ፨ ፩ ፪ ፫ ፬ ፭ ፮ ፯ ፰ ፱ ፲ ፳ ፴ ፵ ፶ ፷ ፸ ፹ ፺ ፻ ፼ ᎀ ᎁ ᎂ ᎃ ᎄ ᎅ ᎆ ᎇ ᎈ ᎉ ᎊ ᎋ ᎌ ᎍ ᎎ ᎏ ᎐ ᎑ ᎒ ᎓ ᎔ ᎕ ᎖ ᎗ ᎘ ᎙\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(chr(x) for x in range(0x1200, 0x139A) if chr(x).isprintable()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 68699,
     "status": "ok",
     "timestamp": 1603688924082,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 420
    },
    "id": "HqfdhaGOWd5a",
    "outputId": "877bb566-c8c9-46c7-9c43-1a882b5c8981"
   },
   "outputs": [],
   "source": [
    "#morphological analysis\n",
    "dirname1=\"FILE\"\n",
    "sor=\"FILE/NEW\"\n",
    "def HornMorpho_stem():                 \n",
    "   words=[]\n",
    "   stem=[]\n",
    "   word=' '\n",
    "   citation=[]\n",
    "   _word=' '\n",
    "   selectedword=''\n",
    "   for fname in os.listdir(dirname1):        \n",
    "    with open(os.path.join(dirname1, fname),encoding='utf8',errors='replace') as f:     \n",
    "     for line in f: \n",
    "       if line== chr(0x000A):  \n",
    "         if len(stem)!=0:  \n",
    "           selectedword= stem[0]\n",
    "#         elif len(citation)!=0 and len(stem)==0:\n",
    "#           selectedword= citation[0]  \n",
    "         elif _word !=' ' and len(citation)==0 and len(stem)==0:\n",
    "           selectedword= _word  \n",
    "         elif word !=' ' and len(citation)==0 and len(stem)==0:\n",
    "           selectedword= word   \n",
    "         stem=[]\n",
    "         word=' '\n",
    "         citation=[]\n",
    "         _word=' '  \n",
    "         words.append(selectedword)\n",
    "       else:\n",
    "         parts = line.rstrip('\\n').split(\" \")\n",
    "         i=0 \n",
    "         for x in parts: \n",
    "           if x=='stem:': \n",
    "              stem.append(parts[i+1]) \n",
    "           elif x=='?word:':\n",
    "              _word=parts[i+1]\n",
    "           elif x=='word:':    \n",
    "              word=parts[i+1]\n",
    "           i=i+1\n",
    "    filename=sor + \"/\" + fname\n",
    "    print(\"processed and saved\",filename)\n",
    "  \n",
    "\n",
    "HornMorpho_stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 68695,
     "status": "ok",
     "timestamp": 1603688924095,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 420
    },
    "id": "CacnExZ0X1lg",
    "outputId": "a2486efe-bdaa-48b3-f2d5-bb866e6b4757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_word removal is processed and saved at FILE/NEW\n"
     ]
    }
   ],
   "source": [
    "#stop word removal\n",
    "import os\n",
    "def stopword_removal():\n",
    "    dirname1=\"amha_wol_nmt.txt\"\n",
    "    dirname=\"FILE\"\n",
    "    dirname2=\"FILE/NEW\"\n",
    "    rep=[]\n",
    "    with open(dirname1, encoding='utf8') as f:\n",
    "         for line in f:\n",
    "            words = line.rstrip('\\n').split('\\t')\n",
    "            #y=words[1]\n",
    "            rep.append(words[0])\n",
    "    f.close\n",
    "    stop_word2=[]\n",
    "    for files in open(dirname1,'r',encoding='utf8'):\n",
    "        for line in files.split('\\n'):\n",
    "            stop_word2.append(line)\n",
    "    rep.extend(stop_word2)\n",
    "    i=0\n",
    "    filedata=[]\n",
    "    filedata=' '\n",
    "\n",
    "    print(\"stop_word removal is processed and saved at\", dirname2)\n",
    "\n",
    "stopword_removal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bQFr4UZX-gYP"
   },
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "  word_pairs = [[w for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "  word_pairs = [[normalize_amharic(w[0]), normalize_wolaita(w[1])]\n",
    "                  for w in word_pairs]\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2131,
     "status": "ok",
     "timestamp": 1605002716130,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "Y2tdF_eA-gac",
    "outputId": "e9d7375d-e176-4464-e36b-cb6cfb5e79cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> እኔ ከሀዋርያት ሁሉ የማንስ ነኝና የእግዚአብሄርን ቤተ ክርስቲያን ስላሳደድሁ ሀዋርያ ተብዬ ልጠራ የማይገባኝ <end>\n",
      "<start> aissi giikko, yesuusi kiittido ubbaappe taani laafa taani xoossaa woosa keettaa waissido gishshau, yesuusi kiittidoogaa geetettada xeesettanaukka bessikke. <end>\n"
     ]
    }
   ],
   "source": [
    "#here is sample output at index one after preproccessing the input corpus\n",
    "Amharic, Wolaita = create_dataset(path_to_file, None)\n",
    "print(Amharic[25])\n",
    "print(Wolaita[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-nztPFp--gbh"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "  return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "otPilD8R-gc3"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e4QVjrRm-gdZ"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  Amharic_lang, Wolaita_lang = create_dataset(path, num_examples)\n",
    "  amharic_tensor, Amharic_lang_tokenizer = tokenize(Amharic_lang)#tokenize and return tensor and tokenizer for amharic language\n",
    "  wola_tensor, Wolaita_lang_tokenizer = tokenize(Wolaita_lang)\n",
    "  return amharic_tensor, wola_tensor, Amharic_lang_tokenizer, Wolaita_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1437,
     "status": "ok",
     "timestamp": 1605002737419,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "h8ISvGVi6b3d",
    "outputId": "5fa558c2-cd29-445c-c006-f458ae9995c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9280\n"
     ]
    }
   ],
   "source": [
    "lines = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vfpLGFUX-gez"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = len(lines)\n",
    "amharic_tensor, wola_tensor, Amharic_lang, Wolaita_lang = load_dataset(path_to_file, num_examples)\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_wolaita_sentences, max_length_amharic_sentences = max_length(wola_tensor), max_length(amharic_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1605002750344,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "-AMMB8i3-gfR",
    "outputId": "1fdc0fde-4511-4368-c7de-9741907f8f05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 52)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_wolaita_sentences, max_length_amharic_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1045,
     "status": "ok",
     "timestamp": 1605002755472,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "xX4ocfLy-ggx",
    "outputId": "1f2a5c2e-eea5-4f8e-da1a-e6f035176cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7424 7424 1856 1856\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "amharic_tensor_train, amharic_tensor_val, wolaita_tensor_train, wolaita_tensorr_val = train_test_split(amharic_tensor, wola_tensor, test_size=0.2)\n",
    "print(len(amharic_tensor_train), len(wolaita_tensor_train), len(amharic_tensor_val), len(wolaita_tensorr_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TVdiMA2o-ghJ"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1235,
     "status": "ok",
     "timestamp": 1605002787073,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "LtwqKDip-ghr",
    "outputId": "acbe8d9b-8104-43ab-dcc6-890a0d20d847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input(Amharic) Language; index to word mapping\n",
      "1 ----> <start>\n",
      "289 ----> ከዚያም\n",
      "1576 ----> መንፈሱ\n",
      "686 ----> ፍላጎት\n",
      "23949 ----> እንዲያድርባችሁም\n",
      "36 ----> ሆነ\n",
      "23950 ----> ለተግባር\n",
      "9951 ----> እንድትነሳሱ\n",
      "533 ----> በማድረግ\n",
      "216 ----> ሀይል\n",
      "5255 ----> የሚሰጣችሁ\n",
      "51 ----> እንዴት\n",
      "122 ----> እንደሆነ\n",
      "3709 ----> ለማስተዋል\n",
      "9846 ----> ሞክሩ\n",
      "2 ----> <end>\n",
      "Target(Wolaita) Language; index to word mapping\n",
      "1 ----> <start>\n",
      "1443 ----> yaatada\n",
      "4 ----> a\n",
      "1067 ----> ayyaanay\n",
      "27 ----> neeni\n",
      "1047 ----> eeno\n",
      "19108 ----> gaanaadaaninne\n",
      "431 ----> oottanaadan\n",
      "145 ----> neeyyo\n",
      "288 ----> waati\n",
      "318 ----> wolqqaa\n",
      "19109 ----> immiyaakko\n",
      "1532 ----> akeeka.\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input(Amharic) Language; index to word mapping\")\n",
    "convert(Amharic_lang, amharic_tensor_train[1])\n",
    "print (\"Target(Wolaita) Language; index to word mapping\")\n",
    "convert(Wolaita_lang, wolaita_tensor_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kFg0Q3QL-gig"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(amharic_tensor_train)\n",
    "BATCH_SIZE = 64 \n",
    "steps_per_epoch = len(amharic_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(Amharic_lang.word_index)+1\n",
    "vocab_tar_size = len(Wolaita_lang.word_index)+1\n",
    "dataset = tf.data.Dataset.from_tensor_slices((amharic_tensor_train, wolaita_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1191,
     "status": "ok",
     "timestamp": 1605002816336,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "ZrX_ssrb-gix",
    "outputId": "3a40facf-b411-4447-afed-aa5e8b425a21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 52]), TensorShape([64, 57]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zwjp0M1P-gjj"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5235,
     "status": "ok",
     "timestamp": 1605002833401,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "YtHSLws2-gj2",
    "outputId": "2c806f67-7b43-423e-86a5-97ac7f80e11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 52, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uSDpu2pO-glC"
   },
   "outputs": [],
   "source": [
    "class Attention_Layer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(Attention_Layer, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "  def call(self, query, values):\n",
    "    # hidden shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # we are doing this to perform addition to calculate the score\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2108,
     "status": "ok",
     "timestamp": 1605002846805,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "2DtmIx-Q-glZ",
    "outputId": "5a062ff0-9820-444d-9821-4572fd45f462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 52, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = Attention_Layer(2)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iqxGuw4O-glp"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "    # used for attention\n",
    "    self.attention = Attention_Layer(self.dec_units)\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1327,
     "status": "ok",
     "timestamp": 1605002859193,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "n3fe92DW-gm7",
    "outputId": "59fee3bc-337b-4153-b30a-df44d596e5b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 22088)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),sample_hidden, sample_output)\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "906ESINL-gnL"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "IRiDiKum-gnv"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ZEpV3aGJ-gn_"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([Wolaita_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 629788,
     "status": "ok",
     "timestamp": 1605004335689,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "x3au7xoE-goP",
    "outputId": "31582c98-cca2-4587-fd10-77607df68e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.8325\n",
      "Epoch 1 Batch 1 Loss 2.7952\n",
      "Epoch 1 Batch 2 Loss 2.6454\n",
      "Epoch 1 Batch 3 Loss 2.7850\n",
      "Epoch 1 Batch 4 Loss 2.2609\n",
      "Epoch 1 Batch 5 Loss 2.5219\n",
      "Epoch 1 Batch 6 Loss 2.1820\n",
      "Epoch 1 Batch 7 Loss 2.0980\n",
      "Epoch 1 Batch 8 Loss 2.2777\n",
      "Epoch 1 Batch 9 Loss 2.3069\n",
      "Epoch 1 Batch 10 Loss 2.3178\n",
      "Epoch 1 Batch 11 Loss 2.1334\n",
      "Epoch 1 Batch 12 Loss 2.2630\n",
      "Epoch 1 Batch 13 Loss 2.2518\n",
      "Epoch 1 Batch 14 Loss 2.1437\n",
      "Epoch 1 Batch 15 Loss 2.4386\n",
      "Epoch 1 Batch 16 Loss 2.2181\n",
      "Epoch 1 Batch 17 Loss 2.4051\n",
      "Epoch 1 Batch 18 Loss 2.2128\n",
      "Epoch 1 Batch 19 Loss 2.3183\n",
      "Epoch 1 Batch 20 Loss 2.3298\n",
      "Epoch 1 Batch 21 Loss 2.4253\n",
      "Epoch 1 Batch 22 Loss 2.3813\n",
      "Epoch 1 Batch 23 Loss 2.3188\n",
      "Epoch 1 Batch 24 Loss 2.2618\n",
      "Epoch 1 Batch 25 Loss 2.3350\n",
      "Epoch 1 Batch 26 Loss 2.1221\n",
      "Epoch 1 Batch 27 Loss 2.1628\n",
      "Epoch 1 Batch 28 Loss 2.1539\n",
      "Epoch 1 Batch 29 Loss 2.3330\n",
      "Epoch 1 Batch 30 Loss 2.2776\n",
      "Epoch 1 Batch 31 Loss 2.3543\n",
      "Epoch 1 Batch 32 Loss 2.1405\n",
      "Epoch 1 Batch 33 Loss 2.1704\n",
      "Epoch 1 Batch 34 Loss 2.1900\n",
      "Epoch 1 Batch 35 Loss 2.1735\n",
      "Epoch 1 Batch 36 Loss 2.3218\n",
      "Epoch 1 Batch 37 Loss 2.1566\n",
      "Epoch 1 Batch 38 Loss 2.1906\n",
      "Epoch 1 Batch 39 Loss 2.3532\n",
      "Epoch 1 Batch 40 Loss 2.2201\n",
      "Epoch 1 Batch 41 Loss 2.1820\n",
      "Epoch 1 Batch 42 Loss 2.1787\n",
      "Epoch 1 Batch 43 Loss 2.1738\n",
      "Epoch 1 Batch 44 Loss 2.2465\n",
      "Epoch 1 Batch 45 Loss 2.3105\n",
      "Epoch 1 Batch 46 Loss 2.5053\n",
      "Epoch 1 Batch 47 Loss 2.3512\n",
      "Epoch 1 Batch 48 Loss 2.2401\n",
      "Epoch 1 Batch 49 Loss 2.2508\n",
      "Epoch 1 Batch 50 Loss 2.3165\n",
      "Epoch 1 Batch 51 Loss 2.1457\n",
      "Epoch 1 Batch 52 Loss 2.3286\n",
      "Epoch 1 Batch 53 Loss 2.3142\n",
      "Epoch 1 Batch 54 Loss 2.3361\n",
      "Epoch 1 Batch 55 Loss 2.4372\n",
      "Epoch 1 Batch 56 Loss 2.4066\n",
      "Epoch 1 Batch 57 Loss 2.2022\n",
      "Epoch 1 Batch 58 Loss 2.5032\n",
      "Epoch 1 Batch 59 Loss 2.2913\n",
      "Epoch 1 Batch 60 Loss 2.0990\n",
      "Epoch 1 Batch 61 Loss 2.2972\n",
      "Epoch 1 Batch 62 Loss 2.2117\n",
      "Epoch 1 Batch 63 Loss 2.4291\n",
      "Epoch 1 Batch 64 Loss 2.2912\n",
      "Epoch 1 Batch 65 Loss 2.2680\n",
      "Epoch 1 Batch 66 Loss 2.0802\n",
      "Epoch 1 Batch 67 Loss 1.9055\n",
      "Epoch 1 Batch 68 Loss 2.1950\n",
      "Epoch 1 Batch 69 Loss 2.3584\n",
      "Epoch 1 Batch 70 Loss 2.4827\n",
      "Epoch 1 Batch 71 Loss 2.3724\n",
      "Epoch 1 Batch 72 Loss 2.3222\n",
      "Epoch 1 Batch 73 Loss 2.0946\n",
      "Epoch 1 Batch 74 Loss 2.2283\n",
      "Epoch 1 Batch 75 Loss 2.2659\n",
      "Epoch 1 Batch 76 Loss 2.1187\n",
      "Epoch 1 Batch 77 Loss 2.2250\n",
      "Epoch 1 Batch 78 Loss 2.1974\n",
      "Epoch 1 Batch 79 Loss 2.1270\n",
      "Epoch 1 Batch 80 Loss 2.1215\n",
      "Epoch 1 Batch 81 Loss 2.3046\n",
      "Epoch 1 Batch 82 Loss 2.3708\n",
      "Epoch 1 Batch 83 Loss 2.1326\n",
      "Epoch 1 Batch 84 Loss 2.1507\n",
      "Epoch 1 Batch 85 Loss 2.2971\n",
      "Epoch 1 Batch 86 Loss 2.0847\n",
      "Epoch 1 Batch 87 Loss 2.1478\n",
      "Epoch 1 Batch 88 Loss 2.1086\n",
      "Epoch 1 Batch 89 Loss 2.3134\n",
      "Epoch 1 Batch 90 Loss 2.4209\n",
      "Epoch 1 Batch 91 Loss 2.2712\n",
      "Epoch 1 Batch 92 Loss 2.1674\n",
      "Epoch 1 Batch 93 Loss 2.3087\n",
      "Epoch 1 Batch 94 Loss 2.3184\n",
      "Epoch 1 Batch 95 Loss 2.1924\n",
      "Epoch 1 Batch 96 Loss 2.1495\n",
      "Epoch 1 Batch 97 Loss 2.3421\n",
      "Epoch 1 Batch 98 Loss 2.0408\n",
      "Epoch 1 Batch 99 Loss 2.2024\n",
      "Epoch 1 Batch 100 Loss 2.1267\n",
      "Epoch 1 Batch 101 Loss 2.1894\n",
      "Epoch 1 Batch 102 Loss 2.3674\n",
      "Epoch 1 Batch 103 Loss 2.2930\n",
      "Epoch 1 Batch 104 Loss 2.0766\n",
      "Epoch 1 Batch 105 Loss 2.0377\n",
      "Epoch 1 Batch 106 Loss 2.3297\n",
      "Epoch 1 Batch 107 Loss 2.3111\n",
      "Epoch 1 Batch 108 Loss 2.3504\n",
      "Epoch 1 Batch 109 Loss 2.2306\n",
      "Epoch 1 Batch 110 Loss 2.2496\n",
      "Epoch 1 Batch 111 Loss 2.2873\n",
      "Epoch 1 Batch 112 Loss 2.1199\n",
      "Epoch 1 Batch 113 Loss 2.1451\n",
      "Epoch 1 Batch 114 Loss 2.1822\n",
      "Epoch 1 Batch 115 Loss 1.9631\n",
      "Epoch 1 Loss 2.2653\n",
      "Time taken for this epoch 3023.0764 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.1251\n",
      "Epoch 2 Batch 1 Loss 2.0255\n",
      "Epoch 2 Batch 2 Loss 2.0052\n",
      "Epoch 2 Batch 3 Loss 2.0852\n",
      "Epoch 2 Batch 4 Loss 2.0554\n",
      "Epoch 2 Batch 5 Loss 2.1263\n",
      "Epoch 2 Batch 6 Loss 2.2294\n",
      "Epoch 2 Batch 7 Loss 2.0376\n",
      "Epoch 2 Batch 8 Loss 2.0054\n",
      "Epoch 2 Batch 9 Loss 2.0598\n",
      "Epoch 2 Batch 10 Loss 2.3514\n",
      "Epoch 2 Batch 11 Loss 2.2052\n",
      "Epoch 2 Batch 12 Loss 1.9938\n",
      "Epoch 2 Batch 13 Loss 1.9961\n",
      "Epoch 2 Batch 14 Loss 2.0103\n",
      "Epoch 2 Batch 15 Loss 2.0921\n",
      "Epoch 2 Batch 16 Loss 2.1863\n",
      "Epoch 2 Batch 17 Loss 2.2550\n",
      "Epoch 2 Batch 18 Loss 2.1565\n",
      "Epoch 2 Batch 19 Loss 2.0283\n",
      "Epoch 2 Batch 20 Loss 2.1139\n",
      "Epoch 2 Batch 21 Loss 2.1383\n",
      "Epoch 2 Batch 22 Loss 1.9911\n",
      "Epoch 2 Batch 23 Loss 2.2253\n",
      "Epoch 2 Batch 24 Loss 2.2308\n",
      "Epoch 2 Batch 25 Loss 1.9853\n",
      "Epoch 2 Batch 26 Loss 2.1825\n",
      "Epoch 2 Batch 27 Loss 2.0848\n",
      "Epoch 2 Batch 28 Loss 2.2667\n",
      "Epoch 2 Batch 29 Loss 2.1115\n",
      "Epoch 2 Batch 30 Loss 2.2683\n",
      "Epoch 2 Batch 31 Loss 2.1791\n",
      "Epoch 2 Batch 32 Loss 1.9793\n",
      "Epoch 2 Batch 33 Loss 2.2110\n",
      "Epoch 2 Batch 34 Loss 2.0026\n",
      "Epoch 2 Batch 35 Loss 2.0416\n",
      "Epoch 2 Batch 36 Loss 1.9599\n",
      "Epoch 2 Batch 37 Loss 2.1950\n",
      "Epoch 2 Batch 38 Loss 2.1290\n",
      "Epoch 2 Batch 39 Loss 2.0246\n",
      "Epoch 2 Batch 40 Loss 2.3114\n",
      "Epoch 2 Batch 41 Loss 2.0660\n",
      "Epoch 2 Batch 42 Loss 2.2009\n",
      "Epoch 2 Batch 43 Loss 1.9589\n",
      "Epoch 2 Batch 44 Loss 2.3181\n",
      "Epoch 2 Batch 45 Loss 2.0241\n",
      "Epoch 2 Batch 46 Loss 2.1323\n",
      "Epoch 2 Batch 47 Loss 1.9712\n",
      "Epoch 2 Batch 48 Loss 2.2658\n",
      "Epoch 2 Batch 49 Loss 2.0668\n",
      "Epoch 2 Batch 50 Loss 2.0636\n",
      "Epoch 2 Batch 51 Loss 2.1271\n",
      "Epoch 2 Batch 52 Loss 2.0628\n",
      "Epoch 2 Batch 53 Loss 2.1512\n",
      "Epoch 2 Batch 54 Loss 2.2599\n",
      "Epoch 2 Batch 55 Loss 2.1543\n",
      "Epoch 2 Batch 56 Loss 2.0882\n",
      "Epoch 2 Batch 57 Loss 2.0363\n",
      "Epoch 2 Batch 58 Loss 2.1501\n",
      "Epoch 2 Batch 59 Loss 2.1659\n",
      "Epoch 2 Batch 60 Loss 2.3436\n",
      "Epoch 2 Batch 61 Loss 2.1046\n",
      "Epoch 2 Batch 62 Loss 2.2821\n",
      "Epoch 2 Batch 63 Loss 2.2401\n",
      "Epoch 2 Batch 64 Loss 2.0811\n",
      "Epoch 2 Batch 65 Loss 2.0249\n",
      "Epoch 2 Batch 66 Loss 2.0267\n",
      "Epoch 2 Batch 67 Loss 2.4024\n",
      "Epoch 2 Batch 68 Loss 2.0278\n",
      "Epoch 2 Batch 69 Loss 2.2103\n",
      "Epoch 2 Batch 70 Loss 2.0826\n",
      "Epoch 2 Batch 71 Loss 2.0661\n",
      "Epoch 2 Batch 72 Loss 1.9940\n",
      "Epoch 2 Batch 73 Loss 1.9827\n",
      "Epoch 2 Batch 74 Loss 2.1544\n",
      "Epoch 2 Batch 75 Loss 2.1462\n",
      "Epoch 2 Batch 76 Loss 2.0542\n",
      "Epoch 2 Batch 77 Loss 1.9820\n",
      "Epoch 2 Batch 78 Loss 2.0632\n",
      "Epoch 2 Batch 79 Loss 2.1362\n",
      "Epoch 2 Batch 80 Loss 2.1780\n",
      "Epoch 2 Batch 81 Loss 1.9928\n",
      "Epoch 2 Batch 82 Loss 2.1069\n",
      "Epoch 2 Batch 83 Loss 2.1842\n",
      "Epoch 2 Batch 84 Loss 2.0319\n",
      "Epoch 2 Batch 85 Loss 1.9909\n",
      "Epoch 2 Batch 86 Loss 2.0223\n",
      "Epoch 2 Batch 87 Loss 2.1500\n",
      "Epoch 2 Batch 88 Loss 2.0629\n",
      "Epoch 2 Batch 89 Loss 2.1475\n",
      "Epoch 2 Batch 90 Loss 2.1582\n",
      "Epoch 2 Batch 91 Loss 2.3030\n",
      "Epoch 2 Batch 92 Loss 2.1710\n",
      "Epoch 2 Batch 93 Loss 2.0079\n",
      "Epoch 2 Batch 94 Loss 2.0282\n",
      "Epoch 2 Batch 95 Loss 2.0627\n",
      "Epoch 2 Batch 96 Loss 2.1488\n",
      "Epoch 2 Batch 97 Loss 1.9774\n",
      "Epoch 2 Batch 98 Loss 2.1779\n",
      "Epoch 2 Batch 99 Loss 2.0999\n",
      "Epoch 2 Batch 100 Loss 1.8852\n",
      "Epoch 2 Batch 101 Loss 2.1732\n",
      "Epoch 2 Batch 102 Loss 2.2348\n",
      "Epoch 2 Batch 103 Loss 2.0435\n",
      "Epoch 2 Batch 104 Loss 1.9141\n",
      "Epoch 2 Batch 105 Loss 2.1457\n",
      "Epoch 2 Batch 106 Loss 2.0285\n",
      "Epoch 2 Batch 107 Loss 2.0792\n",
      "Epoch 2 Batch 108 Loss 2.2355\n",
      "Epoch 2 Batch 109 Loss 2.3901\n",
      "Epoch 2 Batch 110 Loss 1.9770\n",
      "Epoch 2 Batch 111 Loss 2.1903\n",
      "Epoch 2 Batch 112 Loss 2.2304\n",
      "Epoch 2 Batch 113 Loss 2.0288\n",
      "Epoch 2 Batch 114 Loss 1.9320\n",
      "Epoch 2 Batch 115 Loss 2.1134\n",
      "Epoch 2 Loss 2.1115\n",
      "Time taken for this epoch 2890.4792 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.0734\n",
      "Epoch 3 Batch 1 Loss 1.9895\n",
      "Epoch 3 Batch 2 Loss 2.0480\n",
      "Epoch 3 Batch 3 Loss 1.9845\n",
      "Epoch 3 Batch 4 Loss 1.9641\n",
      "Epoch 3 Batch 5 Loss 1.9730\n",
      "Epoch 3 Batch 6 Loss 2.1270\n",
      "Epoch 3 Batch 7 Loss 2.0042\n",
      "Epoch 3 Batch 8 Loss 2.1833\n",
      "Epoch 3 Batch 9 Loss 1.8358\n",
      "Epoch 3 Batch 10 Loss 1.9581\n",
      "Epoch 3 Batch 11 Loss 1.7531\n",
      "Epoch 3 Batch 12 Loss 2.4228\n",
      "Epoch 3 Batch 13 Loss 2.0415\n",
      "Epoch 3 Batch 14 Loss 2.0359\n",
      "Epoch 3 Batch 15 Loss 2.1448\n",
      "Epoch 3 Batch 16 Loss 2.1634\n",
      "Epoch 3 Batch 17 Loss 2.2184\n",
      "Epoch 3 Batch 18 Loss 2.0786\n",
      "Epoch 3 Batch 19 Loss 2.1823\n",
      "Epoch 3 Batch 20 Loss 1.9676\n",
      "Epoch 3 Batch 21 Loss 1.9003\n",
      "Epoch 3 Batch 22 Loss 2.1442\n",
      "Epoch 3 Batch 23 Loss 2.0454\n",
      "Epoch 3 Batch 24 Loss 2.0058\n",
      "Epoch 3 Batch 25 Loss 2.2048\n",
      "Epoch 3 Batch 26 Loss 2.0805\n",
      "Epoch 3 Batch 27 Loss 1.9540\n",
      "Epoch 3 Batch 28 Loss 2.0412\n",
      "Epoch 3 Batch 29 Loss 2.2035\n",
      "Epoch 3 Batch 30 Loss 2.1768\n",
      "Epoch 3 Batch 31 Loss 1.9541\n",
      "Epoch 3 Batch 32 Loss 2.0928\n",
      "Epoch 3 Batch 33 Loss 1.9898\n",
      "Epoch 3 Batch 34 Loss 2.1960\n",
      "Epoch 3 Batch 35 Loss 2.3185\n",
      "Epoch 3 Batch 36 Loss 2.0946\n",
      "Epoch 3 Batch 37 Loss 2.1968\n",
      "Epoch 3 Batch 38 Loss 1.8675\n",
      "Epoch 3 Batch 39 Loss 1.8057\n",
      "Epoch 3 Batch 40 Loss 1.9479\n",
      "Epoch 3 Batch 41 Loss 2.2392\n",
      "Epoch 3 Batch 42 Loss 1.9682\n",
      "Epoch 3 Batch 43 Loss 1.8544\n",
      "Epoch 3 Batch 44 Loss 1.9451\n",
      "Epoch 3 Batch 45 Loss 2.1973\n",
      "Epoch 3 Batch 46 Loss 2.0416\n",
      "Epoch 3 Batch 47 Loss 2.0682\n",
      "Epoch 3 Batch 48 Loss 1.9624\n",
      "Epoch 3 Batch 49 Loss 1.9315\n",
      "Epoch 3 Batch 50 Loss 1.9657\n",
      "Epoch 3 Batch 51 Loss 2.0335\n",
      "Epoch 3 Batch 52 Loss 2.1158\n",
      "Epoch 3 Batch 53 Loss 2.2034\n",
      "Epoch 3 Batch 54 Loss 2.0267\n",
      "Epoch 3 Batch 55 Loss 2.1511\n",
      "Epoch 3 Batch 56 Loss 1.9682\n",
      "Epoch 3 Batch 57 Loss 2.0149\n",
      "Epoch 3 Batch 58 Loss 2.1101\n",
      "Epoch 3 Batch 59 Loss 2.2171\n",
      "Epoch 3 Batch 60 Loss 2.0324\n",
      "Epoch 3 Batch 61 Loss 2.0373\n",
      "Epoch 3 Batch 62 Loss 2.1010\n",
      "Epoch 3 Batch 63 Loss 1.9664\n",
      "Epoch 3 Batch 64 Loss 2.2869\n",
      "Epoch 3 Batch 65 Loss 2.1706\n",
      "Epoch 3 Batch 66 Loss 1.8900\n",
      "Epoch 3 Batch 67 Loss 1.9205\n",
      "Epoch 3 Batch 68 Loss 1.9254\n",
      "Epoch 3 Batch 69 Loss 2.1625\n",
      "Epoch 3 Batch 70 Loss 2.1398\n",
      "Epoch 3 Batch 71 Loss 1.8667\n",
      "Epoch 3 Batch 72 Loss 1.8815\n",
      "Epoch 3 Batch 73 Loss 1.9988\n",
      "Epoch 3 Batch 74 Loss 2.0289\n",
      "Epoch 3 Batch 75 Loss 2.0686\n",
      "Epoch 3 Batch 76 Loss 2.0532\n",
      "Epoch 3 Batch 77 Loss 2.0451\n",
      "Epoch 3 Batch 78 Loss 2.0392\n",
      "Epoch 3 Batch 79 Loss 2.1720\n",
      "Epoch 3 Batch 80 Loss 2.1294\n",
      "Epoch 3 Batch 81 Loss 1.8668\n",
      "Epoch 3 Batch 82 Loss 2.2143\n",
      "Epoch 3 Batch 83 Loss 1.9027\n",
      "Epoch 3 Batch 84 Loss 2.1140\n",
      "Epoch 3 Batch 85 Loss 2.1607\n",
      "Epoch 3 Batch 86 Loss 1.9492\n",
      "Epoch 3 Batch 87 Loss 1.9543\n",
      "Epoch 3 Batch 88 Loss 2.0686\n",
      "Epoch 3 Batch 89 Loss 1.9561\n",
      "Epoch 3 Batch 90 Loss 2.1375\n",
      "Epoch 3 Batch 91 Loss 2.1560\n",
      "Epoch 3 Batch 92 Loss 2.0968\n",
      "Epoch 3 Batch 93 Loss 1.8833\n",
      "Epoch 3 Batch 94 Loss 1.9287\n",
      "Epoch 3 Batch 95 Loss 1.9556\n",
      "Epoch 3 Batch 96 Loss 2.0042\n",
      "Epoch 3 Batch 97 Loss 2.2927\n",
      "Epoch 3 Batch 98 Loss 1.9783\n",
      "Epoch 3 Batch 99 Loss 2.0586\n",
      "Epoch 3 Batch 100 Loss 1.9321\n",
      "Epoch 3 Batch 101 Loss 2.0306\n",
      "Epoch 3 Batch 102 Loss 2.1979\n",
      "Epoch 3 Batch 103 Loss 2.2448\n",
      "Epoch 3 Batch 104 Loss 1.8616\n",
      "Epoch 3 Batch 105 Loss 2.2603\n",
      "Epoch 3 Batch 106 Loss 1.9875\n",
      "Epoch 3 Batch 107 Loss 2.1365\n",
      "Epoch 3 Batch 108 Loss 1.9286\n",
      "Epoch 3 Batch 109 Loss 2.1542\n",
      "Epoch 3 Batch 110 Loss 2.2506\n",
      "Epoch 3 Batch 111 Loss 2.0384\n",
      "Epoch 3 Batch 112 Loss 1.9953\n",
      "Epoch 3 Batch 113 Loss 2.0745\n",
      "Epoch 3 Batch 114 Loss 1.9031\n",
      "Epoch 3 Batch 115 Loss 1.9951\n",
      "Epoch 3 Loss 2.0515\n",
      "Time taken for this epoch 2888.7437 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.0803\n",
      "Epoch 4 Batch 1 Loss 1.9536\n",
      "Epoch 4 Batch 2 Loss 2.2585\n",
      "Epoch 4 Batch 3 Loss 2.0002\n",
      "Epoch 4 Batch 4 Loss 1.9878\n",
      "Epoch 4 Batch 5 Loss 2.0286\n",
      "Epoch 4 Batch 6 Loss 1.9615\n",
      "Epoch 4 Batch 7 Loss 2.0421\n",
      "Epoch 4 Batch 8 Loss 1.8779\n",
      "Epoch 4 Batch 9 Loss 2.0875\n",
      "Epoch 4 Batch 10 Loss 2.0321\n",
      "Epoch 4 Batch 11 Loss 1.9712\n",
      "Epoch 4 Batch 12 Loss 1.9525\n",
      "Epoch 4 Batch 13 Loss 1.9701\n",
      "Epoch 4 Batch 14 Loss 1.9871\n",
      "Epoch 4 Batch 15 Loss 2.0184\n",
      "Epoch 4 Batch 16 Loss 1.9087\n",
      "Epoch 4 Batch 17 Loss 1.9827\n",
      "Epoch 4 Batch 18 Loss 1.8800\n",
      "Epoch 4 Batch 19 Loss 2.1372\n",
      "Epoch 4 Batch 20 Loss 1.8706\n",
      "Epoch 4 Batch 21 Loss 2.0217\n",
      "Epoch 4 Batch 22 Loss 1.8661\n",
      "Epoch 4 Batch 23 Loss 1.7457\n",
      "Epoch 4 Batch 24 Loss 2.0371\n",
      "Epoch 4 Batch 25 Loss 1.8718\n",
      "Epoch 4 Batch 26 Loss 2.0497\n",
      "Epoch 4 Batch 27 Loss 1.7619\n",
      "Epoch 4 Batch 28 Loss 2.1546\n",
      "Epoch 4 Batch 29 Loss 1.8729\n",
      "Epoch 4 Batch 30 Loss 2.0274\n",
      "Epoch 4 Batch 31 Loss 2.0106\n",
      "Epoch 4 Batch 32 Loss 2.0641\n",
      "Epoch 4 Batch 33 Loss 1.9656\n",
      "Epoch 4 Batch 34 Loss 2.0075\n",
      "Epoch 4 Batch 35 Loss 1.8521\n",
      "Epoch 4 Batch 36 Loss 1.8947\n",
      "Epoch 4 Batch 37 Loss 2.0070\n",
      "Epoch 4 Batch 38 Loss 1.9770\n",
      "Epoch 4 Batch 39 Loss 2.0487\n",
      "Epoch 4 Batch 40 Loss 2.0822\n",
      "Epoch 4 Batch 41 Loss 2.0979\n",
      "Epoch 4 Batch 42 Loss 1.9400\n",
      "Epoch 4 Batch 43 Loss 2.1772\n",
      "Epoch 4 Batch 44 Loss 2.0288\n",
      "Epoch 4 Batch 45 Loss 1.8493\n",
      "Epoch 4 Batch 46 Loss 2.0412\n",
      "Epoch 4 Batch 47 Loss 2.0679\n",
      "Epoch 4 Batch 48 Loss 1.8423\n",
      "Epoch 4 Batch 49 Loss 1.9443\n",
      "Epoch 4 Batch 50 Loss 2.0537\n",
      "Epoch 4 Batch 51 Loss 2.1497\n",
      "Epoch 4 Batch 52 Loss 2.0196\n",
      "Epoch 4 Batch 53 Loss 1.9453\n",
      "Epoch 4 Batch 54 Loss 2.1025\n",
      "Epoch 4 Batch 55 Loss 1.9852\n",
      "Epoch 4 Batch 56 Loss 2.1123\n",
      "Epoch 4 Batch 57 Loss 1.9984\n",
      "Epoch 4 Batch 58 Loss 2.1042\n",
      "Epoch 4 Batch 59 Loss 1.9132\n",
      "Epoch 4 Batch 60 Loss 2.0224\n",
      "Epoch 4 Batch 61 Loss 2.1398\n",
      "Epoch 4 Batch 62 Loss 1.9280\n",
      "Epoch 4 Batch 63 Loss 1.6705\n",
      "Epoch 4 Batch 64 Loss 2.0662\n",
      "Epoch 4 Batch 65 Loss 1.8072\n",
      "Epoch 4 Batch 66 Loss 2.1377\n",
      "Epoch 4 Batch 67 Loss 2.0023\n",
      "Epoch 4 Batch 68 Loss 2.0110\n",
      "Epoch 4 Batch 69 Loss 2.0613\n",
      "Epoch 4 Batch 70 Loss 2.1662\n",
      "Epoch 4 Batch 71 Loss 2.1397\n",
      "Epoch 4 Batch 72 Loss 1.9481\n",
      "Epoch 4 Batch 73 Loss 1.9594\n",
      "Epoch 4 Batch 74 Loss 1.7975\n",
      "Epoch 4 Batch 75 Loss 1.8631\n",
      "Epoch 4 Batch 76 Loss 1.9907\n",
      "Epoch 4 Batch 77 Loss 1.9277\n",
      "Epoch 4 Batch 78 Loss 1.7707\n",
      "Epoch 4 Batch 79 Loss 1.8149\n",
      "Epoch 4 Batch 80 Loss 1.7061\n",
      "Epoch 4 Batch 81 Loss 1.9544\n",
      "Epoch 4 Batch 82 Loss 1.9434\n",
      "Epoch 4 Batch 83 Loss 2.0079\n",
      "Epoch 4 Batch 84 Loss 1.9044\n",
      "Epoch 4 Batch 85 Loss 1.9573\n",
      "Epoch 4 Batch 86 Loss 2.1100\n",
      "Epoch 4 Batch 87 Loss 1.7672\n",
      "Epoch 4 Batch 88 Loss 2.0402\n",
      "Epoch 4 Batch 89 Loss 1.8452\n",
      "Epoch 4 Batch 90 Loss 2.1271\n",
      "Epoch 4 Batch 91 Loss 2.0222\n",
      "Epoch 4 Batch 92 Loss 2.1869\n",
      "Epoch 4 Batch 93 Loss 1.9295\n",
      "Epoch 4 Batch 94 Loss 2.0571\n",
      "Epoch 4 Batch 95 Loss 2.0911\n",
      "Epoch 4 Batch 96 Loss 1.9958\n",
      "Epoch 4 Batch 97 Loss 1.8446\n",
      "Epoch 4 Batch 98 Loss 2.1432\n",
      "Epoch 4 Batch 99 Loss 2.0052\n",
      "Epoch 4 Batch 100 Loss 1.9441\n",
      "Epoch 4 Batch 101 Loss 1.8771\n",
      "Epoch 4 Batch 102 Loss 1.8238\n",
      "Epoch 4 Batch 103 Loss 1.9548\n",
      "Epoch 4 Batch 104 Loss 1.9619\n",
      "Epoch 4 Batch 105 Loss 1.8470\n",
      "Epoch 4 Batch 106 Loss 1.9830\n",
      "Epoch 4 Batch 107 Loss 2.1494\n",
      "Epoch 4 Batch 108 Loss 2.0917\n",
      "Epoch 4 Batch 109 Loss 1.9334\n",
      "Epoch 4 Batch 110 Loss 1.8925\n",
      "Epoch 4 Batch 111 Loss 1.8528\n",
      "Epoch 4 Batch 112 Loss 1.9893\n",
      "Epoch 4 Batch 113 Loss 1.9434\n",
      "Epoch 4 Batch 114 Loss 2.0590\n",
      "Epoch 4 Batch 115 Loss 1.8296\n",
      "Epoch 4 Loss 1.9800\n",
      "Time taken for this epoch 2906.5942 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.6817\n",
      "Epoch 5 Batch 1 Loss 1.7558\n",
      "Epoch 5 Batch 2 Loss 1.8739\n",
      "Epoch 5 Batch 3 Loss 2.0596\n",
      "Epoch 5 Batch 4 Loss 1.7774\n",
      "Epoch 5 Batch 5 Loss 1.9295\n",
      "Epoch 5 Batch 6 Loss 1.8643\n",
      "Epoch 5 Batch 7 Loss 1.7270\n",
      "Epoch 5 Batch 8 Loss 1.9768\n",
      "Epoch 5 Batch 9 Loss 1.7243\n",
      "Epoch 5 Batch 10 Loss 1.9878\n",
      "Epoch 5 Batch 11 Loss 1.8452\n",
      "Epoch 5 Batch 12 Loss 1.8684\n",
      "Epoch 5 Batch 13 Loss 2.0261\n",
      "Epoch 5 Batch 14 Loss 1.8780\n",
      "Epoch 5 Batch 15 Loss 1.9129\n",
      "Epoch 5 Batch 16 Loss 1.7222\n",
      "Epoch 5 Batch 17 Loss 1.7868\n",
      "Epoch 5 Batch 18 Loss 1.9019\n",
      "Epoch 5 Batch 19 Loss 2.1100\n",
      "Epoch 5 Batch 20 Loss 1.8550\n",
      "Epoch 5 Batch 21 Loss 1.8119\n",
      "Epoch 5 Batch 22 Loss 1.9340\n",
      "Epoch 5 Batch 23 Loss 1.8420\n",
      "Epoch 5 Batch 24 Loss 2.0172\n",
      "Epoch 5 Batch 25 Loss 1.9302\n",
      "Epoch 5 Batch 26 Loss 1.8753\n",
      "Epoch 5 Batch 27 Loss 1.6813\n",
      "Epoch 5 Batch 28 Loss 2.0100\n",
      "Epoch 5 Batch 29 Loss 1.8164\n",
      "Epoch 5 Batch 30 Loss 1.7915\n",
      "Epoch 5 Batch 31 Loss 1.7013\n",
      "Epoch 5 Batch 32 Loss 1.9252\n",
      "Epoch 5 Batch 33 Loss 1.9557\n",
      "Epoch 5 Batch 34 Loss 1.8668\n",
      "Epoch 5 Batch 35 Loss 1.8862\n",
      "Epoch 5 Batch 36 Loss 1.8949\n",
      "Epoch 5 Batch 37 Loss 1.8495\n",
      "Epoch 5 Batch 38 Loss 2.0510\n",
      "Epoch 5 Batch 39 Loss 2.0627\n",
      "Epoch 5 Batch 40 Loss 2.1272\n",
      "Epoch 5 Batch 41 Loss 1.8106\n",
      "Epoch 5 Batch 42 Loss 2.0222\n",
      "Epoch 5 Batch 43 Loss 1.9454\n",
      "Epoch 5 Batch 44 Loss 1.9474\n",
      "Epoch 5 Batch 45 Loss 1.8971\n",
      "Epoch 5 Batch 46 Loss 1.7470\n",
      "Epoch 5 Batch 47 Loss 1.9224\n",
      "Epoch 5 Batch 48 Loss 1.8450\n",
      "Epoch 5 Batch 49 Loss 1.8485\n",
      "Epoch 5 Batch 50 Loss 1.9475\n",
      "Epoch 5 Batch 51 Loss 1.8733\n",
      "Epoch 5 Batch 52 Loss 1.7886\n",
      "Epoch 5 Batch 53 Loss 1.8849\n",
      "Epoch 5 Batch 54 Loss 1.9347\n",
      "Epoch 5 Batch 55 Loss 1.7334\n",
      "Epoch 5 Batch 56 Loss 1.8724\n",
      "Epoch 5 Batch 57 Loss 1.7694\n",
      "Epoch 5 Batch 58 Loss 1.8748\n",
      "Epoch 5 Batch 59 Loss 2.0352\n",
      "Epoch 5 Batch 60 Loss 1.9643\n",
      "Epoch 5 Batch 61 Loss 1.8842\n",
      "Epoch 5 Batch 62 Loss 1.7658\n",
      "Epoch 5 Batch 63 Loss 1.9494\n",
      "Epoch 5 Batch 64 Loss 2.1068\n",
      "Epoch 5 Batch 65 Loss 1.9383\n",
      "Epoch 5 Batch 66 Loss 1.8278\n",
      "Epoch 5 Batch 67 Loss 1.9507\n",
      "Epoch 5 Batch 68 Loss 1.9445\n",
      "Epoch 5 Batch 69 Loss 1.9619\n",
      "Epoch 5 Batch 70 Loss 1.8783\n",
      "Epoch 5 Batch 71 Loss 2.0059\n",
      "Epoch 5 Batch 72 Loss 1.9843\n",
      "Epoch 5 Batch 73 Loss 1.7581\n",
      "Epoch 5 Batch 74 Loss 1.8708\n",
      "Epoch 5 Batch 75 Loss 1.8125\n",
      "Epoch 5 Batch 76 Loss 2.0551\n",
      "Epoch 5 Batch 77 Loss 1.8546\n",
      "Epoch 5 Batch 78 Loss 1.9547\n",
      "Epoch 5 Batch 79 Loss 1.8670\n",
      "Epoch 5 Batch 80 Loss 1.8302\n",
      "Epoch 5 Batch 81 Loss 1.9688\n",
      "Epoch 5 Batch 82 Loss 1.8045\n",
      "Epoch 5 Batch 83 Loss 1.9702\n",
      "Epoch 5 Batch 84 Loss 1.9450\n",
      "Epoch 5 Batch 85 Loss 2.0159\n",
      "Epoch 5 Batch 86 Loss 1.7614\n",
      "Epoch 5 Batch 87 Loss 1.9685\n",
      "Epoch 5 Batch 88 Loss 1.8691\n",
      "Epoch 5 Batch 89 Loss 1.8201\n",
      "Epoch 5 Batch 90 Loss 1.9847\n",
      "Epoch 5 Batch 91 Loss 1.8179\n",
      "Epoch 5 Batch 92 Loss 2.0767\n",
      "Epoch 5 Batch 93 Loss 1.8630\n",
      "Epoch 5 Batch 94 Loss 1.8945\n",
      "Epoch 5 Batch 95 Loss 1.9345\n",
      "Epoch 5 Batch 96 Loss 1.7985\n",
      "Epoch 5 Batch 97 Loss 1.9087\n",
      "Epoch 5 Batch 98 Loss 1.7012\n",
      "Epoch 5 Batch 99 Loss 1.7167\n",
      "Epoch 5 Batch 100 Loss 1.8628\n",
      "Epoch 5 Batch 101 Loss 1.9145\n",
      "Epoch 5 Batch 102 Loss 1.9681\n",
      "Epoch 5 Batch 103 Loss 1.8586\n",
      "Epoch 5 Batch 104 Loss 1.6992\n",
      "Epoch 5 Batch 105 Loss 1.8807\n",
      "Epoch 5 Batch 106 Loss 1.7944\n",
      "Epoch 5 Batch 107 Loss 1.7923\n",
      "Epoch 5 Batch 108 Loss 1.7810\n",
      "Epoch 5 Batch 109 Loss 1.8490\n",
      "Epoch 5 Batch 110 Loss 2.0154\n",
      "Epoch 5 Batch 111 Loss 2.0122\n",
      "Epoch 5 Batch 112 Loss 1.8557\n",
      "Epoch 5 Batch 113 Loss 1.8845\n",
      "Epoch 5 Batch 114 Loss 1.8653\n",
      "Epoch 5 Batch 115 Loss 1.8909\n",
      "Epoch 5 Loss 1.8871\n",
      "Time taken for this epoch 2902.4821 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,total_loss / steps_per_epoch))\n",
    "  print('Time taken for this epoch {:.4f} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dFKpaR8J-gpc"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence = normalize_amharic(sentence)\n",
    "  inputs = [Amharic_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_amharic_sentences,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  result = ''\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([Wolaita_lang.word_index['<start>']], 0)\n",
    "  for t in range(max_length_wolaita_sentences):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    result += Wolaita_lang.index_word[predicted_id] + ' '\n",
    "    if Wolaita_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "q-lnpzVq-grS"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "  bleu_score = sentence_bleu(sentence, result, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=None, auto_reweigh=False,)\n",
    "  print('Input in Amharic Sentence: %s' % (sentence))\n",
    "  print('Predicted output in Wolaita Sentence: {}'.format(result))\n",
    "  print(\"BLEU score for this translation: %s\" % (bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1805,
     "status": "ok",
     "timestamp": 1605004354901,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "8HwCaS-W-grq",
    "outputId": "45b5a6d0-f042-487a-9acb-521f1ec6e452"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x295d3621dc0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1666,
     "status": "ok",
     "timestamp": 1605004359154,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 480
    },
    "id": "sy-9WlVdVA4J",
    "outputId": "7b5541ad-123f-4c89-fb4a-bb90b6eec626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input in Amharic Sentence: <start> ሰላም እንዴት ናችሁ <end>\n",
      "Predicted output in Wolaita Sentence: shin taani a bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli \n",
      "BLEU score for this translation: 6.358334175421866e-232\n"
     ]
    }
   ],
   "source": [
    "translate(u'ሰላም እንዴት ናችሁ፡፡')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 1295,
     "status": "ok",
     "timestamp": 1603692282591,
     "user": {
      "displayName": "workineh wogaso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghk9YteshIl7pubdWTVXfwoADq3gIomopP87DEJyA=s64",
      "userId": "08827241504179424723"
     },
     "user_tz": 420
    },
    "id": "0J0YrIyH-gr2",
    "outputId": "c972ff0c-aa69-48a1-fc5a-acfcd2f67f16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input in Amharic Sentence: <start> ይህ ሁለተኛ ቋንቋ ነው <end>\n",
      "Predicted output in Wolaita Sentence: shin nuuni a bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli bolli \n",
      "BLEU score for this translation: 6.013341193643636e-232\n"
     ]
    }
   ],
   "source": [
    "translate(u'ይህ ሁለተኛ ቋንቋ ነው፡፡')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ATTENTION_BASED_AMHARIC_WOLAITA_NMT_COLAB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
